<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>EthanLei</title>
    <link>https://ethanyklei.github.io/zh-cn/</link>
    <description>Recent content on EthanLei</description>
    <image>
      <title>EthanLei</title>
      <url>https://ethanyklei.github.io/images/papermod-cover.png</url>
      <link>https://ethanyklei.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.139.3</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 02 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ethanyklei.github.io/zh-cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Loss计算技巧之Logsumexp</title>
      <link>https://ethanyklei.github.io/zh-cn/blogs/posts/loss%E8%AE%A1%E7%AE%97%E6%8A%80%E5%B7%A7%E4%B9%8Blogsumexp/</link>
      <pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/zh-cn/blogs/posts/loss%E8%AE%A1%E7%AE%97%E6%8A%80%E5%B7%A7%E4%B9%8Blogsumexp/</guid>
      <description>Desc Text.</description>
    </item>
    <item>
      <title>Policy-gradient Algorithem</title>
      <link>https://ethanyklei.github.io/zh-cn/blogs/posts/policy_gradient_algorithem/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/zh-cn/blogs/posts/policy_gradient_algorithem/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Policy-gradient 算法是无模型强化学习算法中的一种，也是Actor-Critic 算法中的Actor。
REINFORCE算法是一个比较基础的&lt;/p&gt;
&lt;h2 id=&#34;算法&#34;&gt;算法&lt;/h2&gt;
&lt;h3 id=&#34;学习目标&#34;&gt;学习目标&lt;/h3&gt;
&lt;p&gt;Policy-gradient 的目标函数定义如下：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
J(\theta)=\mathbb{E}[\sum_{t=0}^{T-1}r_{t+1}]
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;它表示希望学习一个policy能够最大化累计未来回报（cumulative future reward）。
$r_{t+1}$表示在状态 $s_t$时采取动作 $a_t$从环境中获得的回报。 $r_{t+1} = R(s_t,a_t)$， $R(\cdot)$表示回报函数（reward function）
因为这是个最大化问题，所以可以使用梯度上升来优化policy。
$$
\begin{equation}
\theta = \theta + \frac{\delta}{\delta \theta}J(\theta)
\end{equation}
$$
Policy 一般会使用神经网络进行参数化。&lt;/p&gt;
&lt;h3 id=&#34;期望expection&#34;&gt;期望（Expection）&lt;/h3&gt;
&lt;p&gt;在文献中经常出现的是期望符号——之所以使用它，是因为我们想要优化长期未来（预测的）奖励，而这有一定的不确定性。
期望值，也称为期望值或平均值，是通过每个x值及其概率的乘积之和来计算的。
$$
\begin{equation}
\mathbb{E}[f(x)] = \sum_{x}P(x)f(x)
\end{equation}
$$
$P(x)$代表随机变量 $x$出现的概率， $f(x)$代表 $x$的值。&lt;/p&gt;
&lt;h3 id=&#34;推导策略梯度&#34;&gt;推导策略梯度&lt;/h3&gt;
&lt;p&gt;我们根据先前定义的目标函数，我们可以将期望项进行展开：
$$
\begin{equation}
\begin{split}
J(\theta) &amp;amp;= \mathbb{E}[\sum_{t=0}^{T-1}r_{t+1}|\pi_{\theta}] \\
&amp;amp;= \sum_{t=i}^{T-1} P(s_t,a_t|\tau)r_{t+1}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;其中 $i$是轨迹 $\tau$中的任意一个起始点， $P(s_t,a_t|\tau)$是在给定轨迹 $\tau$时，出现 $s_t,a_t$的概率。
对两边同时对policy的参数 $\theta$求导：
$$
\begin{equation}
\begin{split}
\nabla_{\theta}J(\theta) &amp;amp;= \sum_{t=i}^{T-1}\nabla_{\theta}P(s_t,a_t|\tau)r_{t+1} \\
&amp;amp;=\sum_{t=i}^{T-1}P(s_t,a_t|\tau)\frac{\nabla_{\theta}P(s_t,a_t|\tau)}{P(s_t,a_t|\tau)} r_{t+1} \\
&amp;amp;=\sum_{t=i}^{T-1}P(s_t,a_t|\tau)\nabla_{\theta}\log{P(s_t,a_t|\tau)}r_{t+1} \\
&amp;amp; = \mathbb{E}[\sum_{t=i}^{T-1}\nabla_{\theta}\log{P(s_t,a_t|\tau)r_{t+1}}]
\end{split}
\end{equation}
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://ethanyklei.github.io/zh-cn/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/zh-cn/about/</guid>
      <description>&lt;div class=&#34;quote-decorated&#34;&gt;
    &lt;p&gt;一个人应该能够换尿布，策划战争，杀猪，开船，设计房子，写十四行诗，结算账户，砌墙，接脱臼的骨头，安慰濒死的人，服从命令，发布命令，携手合作，独立行动，解数学方程，分析新问题，铲粪，电脑编程，做出可口的饭，善打架，勇敢地死去。只有昆虫才专业化。&lt;/p&gt;
    &lt;span class=&#34;author&#34;&gt;— 罗伯特海因莱因，《时间足够爱你》&lt;/span&gt;
&lt;/div&gt;
&lt;style&gt;
.quote-decorated {
    position: relative;
    max-width: 1000px;
    margin: 20px auto;
    padding: 40px 30px;
    background: #fff;
    border-radius: 15px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.08);
}

.quote-decorated::before,
/* .quote-decorated::after {
    content: &#39;&#34;&#39;;
    position: absolute;
    font-size: 80px;
    color: #000;
    opacity: 0.2;
} */

.quote-decorated::before {
    top: 0;
    left: 10px;
}

.quote-decorated::after {
    bottom: -20px;
    right: 10px;
}

.quote-decorated p {
    font-size: 20px;
    line-height: 1.6;
    color: #333;
    margin: 0 0 15px 0;
    font-family: STKaiti;
}

.quote-decorated .author {
    display: block;
    text-align: right;
    color: #666;
    font-family: STKaiti;
}
&lt;/style&gt;
&lt;p&gt;Hello！我叫雷易锟（Ethan Lei），你也可以叫我 Kunkun，是个跟着感觉活着的人（但是也同样需要🥖和🍺）。
目前，我是小红书社区搜索的一名算法工程师，主要在参与生成式搜索相关的业务。
2017～2021年，我在东北大学攻读学士学位。随后，2021～2024年，我在天津大学&lt;a href=&#34;https://tjunlp-lab.github.io/&#34;&gt;自然语言处理实验室&lt;/a&gt;（TJUNLP）&lt;a href=&#34;https://dyxiong.github.io/&#34;&gt;熊德意&lt;/a&gt;教授的指导下，获得了硕士学位。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>https://ethanyklei.github.io/zh-cn/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/zh-cn/publications/</guid>
      <description>&lt;h2 id=&#34;2024&#34;&gt;2024&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📄 Meizhi Zhong, Xikai Liu, Chen Zhang, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Yan Gao, Yao Hu, Kehai Chen and Min Zhang. &lt;a href=&#34;https://ethanyklei.github.io/zh-cn/&#34;&gt;ZigZagKV: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty&lt;/a&gt;. COLING(2025)&lt;/li&gt;
&lt;li&gt;📄 Meizhi Zhong, Chen Zhang, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Xikai Liu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang. &lt;a href=&#34;https://arxiv.org/abs/2406.13282&#34;&gt;Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective
&lt;/a&gt;. COLING(2025).&lt;/li&gt;
&lt;li&gt;📄 Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://arxiv.org/abs/2408.06273&#34;&gt;FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data&lt;/a&gt;. ArXiv(2024).&lt;/li&gt;
&lt;li&gt;📄 Hao Wang, Zhengshan Xue, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10445811/&#34;&gt;End-to-End Speech Translation with Mutual Knowledge Distillation&lt;/a&gt;. ICASSP (2024).&lt;/li&gt;
&lt;li&gt;📄 Xiaohu Zhao, Haoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0957417424001064&#34;&gt;Regularizing Cross-Attention Learning for End-to-End Speech Translation with ASR and MT Attention Matrices&lt;/a&gt;. Expert Systems with Applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2023&#34;&gt;2023&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📄 &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Zhengshan Xue, Xiaohu Zhao, Haoran Sun, Shaolin Zhu, Xiaodong Lin, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.findings-acl.195.pdf&#34;&gt;CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation&lt;/a&gt;. ACL Findings(2023).&lt;/li&gt;
&lt;li&gt;📄 Shaolin Zhu, Shangjie Li, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.acl-long.751.pdf&#34;&gt;PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation&lt;/a&gt;. ACL(2023).&lt;/li&gt;
&lt;li&gt;📄 Haoran Sun, Xiaohu Zhao, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://arxiv.org/pdf/2310.20456&#34;&gt;Towards a Deep Understanding of Multilingual End-to-End Speech Translation&lt;/a&gt;. EMNLP Findings(2023).&lt;/li&gt;
&lt;li&gt;📄 Xiaohu Zhao, Haoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.394.pdf&#34;&gt;CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation&lt;/a&gt;. EMNLP Findings(2023).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2022&#34;&gt;2022&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📄 &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Yuqi Ren, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2022.coling-1.462.pdf&#34;&gt;CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation&lt;/a&gt;. COLING(2022).&lt;/li&gt;
&lt;li&gt;📄 Haoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9961282&#34;&gt;Multilingual Neural Machine Transliteration with Adaptive Segmentation Schemes&lt;/a&gt;. IALP(2022).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
