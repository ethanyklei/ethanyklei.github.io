<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>EthanLei</title>
    <link>https://ethanyklei.github.io/</link>
    <description>Recent content on EthanLei</description>
    <image>
      <title>EthanLei</title>
      <url>https://ethanyklei.github.io/images/papermod-cover.png</url>
      <link>https://ethanyklei.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.139.3</generator>
    <language>en</language>
    <atom:link href="https://ethanyklei.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About Me</title>
      <link>https://ethanyklei.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/about/</guid>
      <description>&lt;div class=&#34;quote-decorated&#34;&gt;
    &lt;p&gt;Life is like a box of chocolates. You never know what you&#39;re gonna get.&lt;/p&gt;
    &lt;span class=&#34;author&#34;&gt;â€” &#34;Forrest Gump&#34;&lt;/span&gt;
&lt;/div&gt;
&lt;style&gt;
.quote-decorated {
    position: relative;
    max-width: 500px;
    margin: 20px auto;
    padding: 40px 30px;
    background: #fff;
    border-radius: 15px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.08);
}

.quote-decorated::before,
/* .quote-decorated::after {
    content: &#39;&#34;&#39;;
    position: absolute;
    font-size: 80px;
    color: #000;
    opacity: 0.2;
} */

.quote-decorated::before {
    top: 0;
    left: 10px;
}

.quote-decorated::after {
    bottom: -20px;
    right: 10px;
}

.quote-decorated p {
    font-size: 20px;
    line-height: 1.6;
    color: #333;
    margin: 0 0 15px 0;
    font-family: &#34;Nothing You Could Do&#34;;
    font-style: italic;
}

.quote-decorated .author {
    display: block;
    text-align: right;
    color: #666;
    font-style: italic;
}
&lt;/style&gt;
&lt;p&gt;Hello! I&amp;rsquo;m Ethan Lei (é›·æ˜“é”Ÿ), but feel free to call me Kunkun. I&amp;rsquo;m an idealist at heart who believes in the transformative power of technology.
Currently, I&amp;rsquo;m working as an Algorithm Engineer at Xiaohongshu Inc., where I focus on developing AI-powered search products and pioneering AI-native applications.
My academic journey includes a Bachelor&amp;rsquo;s degree from Northeastern University (2017-2021) and a Master&amp;rsquo;s degree from Tianjin University (2021-2023), where I was fortunate to work with &lt;a href=&#34;https://tjunlp-lab.github.io/&#34;&gt;TJUNLP&lt;/a&gt; under the guidance of &lt;a href=&#34;https://dyxiong.github.io/&#34;&gt;Prof. Deiyi Xiong&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>https://ethanyklei.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ethanyklei.github.io/publications/</guid>
      <description>&lt;h2 id=&#34;2024&#34;&gt;2024&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ“„â€ƒMeizhi Zhong, Xikai Liu, Chen Zhang, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Yan Gao, Yao Hu, Kehai Chen and Min Zhang. &lt;a href=&#34;https://ethanyklei.github.io/&#34;&gt;ZigZagKV: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty&lt;/a&gt;. COLING(2025)&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒMeizhi Zhong, Chen Zhang, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Xikai Liu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang. &lt;a href=&#34;https://arxiv.org/abs/2406.13282&#34;&gt;Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective
&lt;/a&gt;. COLING(2025).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒHaoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://arxiv.org/abs/2408.06273&#34;&gt;FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data&lt;/a&gt;. ArXiv(2024).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒHao Wang, Zhengshan Xue, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10445811/&#34;&gt;End-to-End Speech Translation with Mutual Knowledge Distillation&lt;/a&gt;. ICASSP (2024).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒXiaohu Zhao, Haoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0957417424001064&#34;&gt;Regularizing Cross-Attention Learning for End-to-End Speech Translation with ASR and MT Attention Matrices&lt;/a&gt;. Expert Systems with Applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2023&#34;&gt;2023&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ“„â€ƒ&lt;strong&gt;Yikun Lei&lt;/strong&gt;, Zhengshan Xue, Xiaohu Zhao, Haoran Sun, Shaolin Zhu, Xiaodong Lin, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.findings-acl.195.pdf&#34;&gt;CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation&lt;/a&gt;. ACL Findings(2023).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒShaolin Zhu, Shangjie Li, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.acl-long.751.pdf&#34;&gt;PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation&lt;/a&gt;. ACL(2023).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒHaoran Sun, Xiaohu Zhao, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://arxiv.org/pdf/2310.20456&#34;&gt;Towards a Deep Understanding of Multilingual End-to-End Speech Translation&lt;/a&gt;. EMNLP Findings(2023).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒXiaohu Zhao, Haoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Shaolin Zhu, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.394.pdf&#34;&gt;CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation&lt;/a&gt;. EMNLP Findings(2023).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2022&#34;&gt;2022&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ“„â€ƒ&lt;strong&gt;Yikun Lei&lt;/strong&gt;, Yuqi Ren, Deyi Xiong. &lt;a href=&#34;https://aclanthology.org/2022.coling-1.462.pdf&#34;&gt;CoDoNMT: Modeling Cohesion Devices for Document-Level Neural Machine Translation&lt;/a&gt;. COLING(2022).&lt;/li&gt;
&lt;li&gt;ðŸ“„â€ƒHaoran Sun, &lt;strong&gt;Yikun Lei&lt;/strong&gt;, Deyi Xiong. &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9961282&#34;&gt;Multilingual Neural Machine Transliteration with Adaptive Segmentation Schemes&lt;/a&gt;. IALP(2022).&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
